{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Build and Train a Model\n",
    "In this notebook we preprocess the data to get it ready for a model, extend a pre-trained model from Keras, train the model on\n",
    "the x-ray images in our dataset, and evaluate model performance. The goal is the be able to accurately classify pneumonia with a\n",
    "reasonable level of accuracy. A good baseline accuracy for this task is ?, we hope to come close to this baseline\n",
    "and potentially be able to outperform it.\n",
    "\n",
    "TODO: Find the baseline accuracy from the article"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import Necessary Packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "from random import sample\n",
    "from sklearn import model_selection, metrics\n",
    "import tensorflow as tf\n",
    "from skimage import io\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Data\n",
    "In this section we setup the data to get it ready for our model. I parse the `Finding Label` to make create binary\n",
    "classification labels for each type of disease for every observation. I also create a `path` column for each `Image Index`\n",
    "so I can get to the location of the actual images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in the Data\n",
    "all_xray_df = pd.read_csv('/data/Data_Entry_2017.csv')\n",
    "\n",
    "# Convert `Finding Labels` to binary labels for each disease\n",
    "all_labels = np.unique(list(chain(*all_xray_df['Finding Labels'].map(lambda x: x.split('|')).tolist())))\n",
    "all_labels = [x for x in all_labels if len(x)>0]\n",
    "for label in all_labels:\n",
    "    if len(label)>1:\n",
    "        all_xray_df[label] = all_xray_df['Finding Labels'].map(lambda diseases: 1.0 if label in diseases else 0)\n",
    "\n",
    "# Create 'path' column for each 'Image Index'\n",
    "all_image_paths = {os.path.basename(x): x for x in\n",
    "                   glob(os.path.join('/data','images*', '*', '*.png'))}\n",
    "all_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\n",
    "\n",
    "# Check the results\n",
    "print(f'Images Found: {len(all_image_paths)}, Total Observations: {all_xray_df.shape[0]}')\n",
    "all_xray_df.sample(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Validation Sets\n",
    "In this section I create the training and validation sets for the model. I am using an 90/10 split of the data with\n",
    "90% of the positive pneumonia cases in the training set and 10% of the positive pneumonia cases in the validation set.\n",
    "Also for the training set the proportion of positive to negative pneumonia cases will be 50/50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_xray_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-321c498bf99f>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     29\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mtrain_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 31\u001B[1;33m \u001B[0mtrain_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcreate_splits\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mall_xray_df\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0.2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'all_xray_df' is not defined"
     ]
    }
   ],
   "source": [
    "def create_splits(df, validation_percent):\n",
    "    # create original train / validation split\n",
    "    train_data, val_data = model_selection.train_test_split(df, test_size=validation_percent, stratify=df['Pneumonia'])\n",
    "\n",
    "    # Update training set to be 50/50 split\n",
    "    pos_indexes = train_data[train_data.Pneumonia == 1].index.tolist()\n",
    "    neg_indexes = train_data[train_data.Pneumonia == 0].index.tolist()\n",
    "    neg_sample = sample(neg_indexes, len(pos_indexes))\n",
    "    train_data = train_data.loc[pos_indexes+neg_sample]\n",
    "\n",
    "    # Check splits\n",
    "    print(f'Total Pneumonia Cases: {df[df.Pneumonia==1].shape[0]} \\\n",
    "    {(1-validation_percent)*100}% Pneumonia Cases: {int(df[df.Pneumonia==1].shape[0]*(1-validation_percent))} \\\n",
    "    {validation_percent*100}% Pneumonia Cases: {int(df[df.Pneumonia==1].shape[0]*validation_percent)}')\n",
    "    print(f'Pneumonia Cases in Training set: {train_data[train_data.Pneumonia==1].shape[0]} \\\n",
    "    Pneumonia Cases in Validation set: {val_data[val_data.Pneumonia==1].shape[0]}')\n",
    "\n",
    "    print(f'Train Set Size: {train_data.shape[0]}, \\\n",
    "    Pos %: {train_data[train_data.Pneumonia==1].shape[0] / train_data.shape[0] *100}, \\\n",
    "    Neg %: {train_data[train_data.Pneumonia==0].shape[0] / train_data.shape[0] *100}')\n",
    "\n",
    "    print(f'Original Set Size: {df.shape[0]} \\\n",
    "    Pos %: {df[df.Pneumonia==1].shape[0] / df.shape[0] *100} \\\n",
    "    Neg %: {df[df.Pneumonia==0].shape[0] / df.shape[0] *100}')\n",
    "\n",
    "    print(f'Validation Set Size: {val_data.shape[0]}, \\\n",
    "    Pos %: {val_data[val_data.Pneumonia == 1].shape[0] / val_data.shape[0] *100}, \\\n",
    "    Neg % {val_data[val_data.Pneumonia == 0].shape[0] / val_data.shape[0] *100}')\n",
    "\n",
    "    return train_data, val_data\n",
    "\n",
    "train_df, val_df = create_splits(all_xray_df, 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Image Augmentation and Data Generators\n",
    "In this section I set up image augmentation to generate a more diverse training set, and create training and validation\n",
    "set generators to use for model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "# Create Image Data Generator for training set\n",
    "train_idg = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                         horizontal_flip = True,\n",
    "                         vertical_flip = False,\n",
    "                         rotation_range = 0.5,\n",
    "                         shear_range = 0.1,\n",
    "                         zoom_range = 0.15)\n",
    "\n",
    "train_generator = train_idg.flow_from_dataframe(dataframe=train_df, directory=None,\n",
    "                                          x_col='path', y_col='Pneumonia',\n",
    "                                          class_mode = 'binary', target_size=IMG_SIZE, batch_size=24)\n",
    "\n",
    "# Create a generator for the test set\n",
    "validation_idg = ImageDataGenerator(rescale=1.0/255.0)\n",
    "validation_generator = validation_idg.flow_from_dataframe(dataframe=val_df, directory=None,\n",
    "                                                      x_col='path', y_col='Pneumonia',\n",
    "                                                      class_mode='binary', target_size=IMG_SIZE, batch_size=11212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a random batch of training and validation data to make sure everything looks okay\n",
    "# Training Examples\n",
    "t_x, t_y = next(train_generator)\n",
    "fig, m_axs = plt.subplots(4, 4, figsize = (16, 16))\n",
    "for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n",
    "    c_ax.imshow(c_x[:,:,0], cmap = 'bone')\n",
    "    if c_y == 1: \n",
    "        c_ax.set_title('Pneumonia')\n",
    "    else:\n",
    "        c_ax.set_title('No Pneumonia')\n",
    "    c_ax.axis('off')\n",
    "\n",
    "# Validation Examples\n",
    "val_x, val_y = next(validation_generator)\n",
    "fig, m_axs = plt.subplots(2, 2, figsize = (16, 16))\n",
    "for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n",
    "    c_ax.imshow(c_x[:,:,0], cmap = 'bone')\n",
    "    if c_y == 1:\n",
    "        c_ax.set_title('Pneumonia')\n",
    "    else:\n",
    "        c_ax.set_title('No Pneumonia')\n",
    "    c_ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending a Pre-trained Model\n",
    "In this section I use a pre-trained model from Keras called VGG16 to attempt to solve this classification problem.\n",
    "TODO: Add more details after training (Potentially try other model architectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the pre-trained VGG16 Architecture\n",
    "vgg16 = tf.keras.applications.VGG16(include_top=True, weights='imagenet')\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setup our own model that is an extension of VGG16\n",
    "def setup_model(model, input_size, output_size: int, num_active_layers: int, lr):\n",
    "    # Remove the final layer and replace it with our own classification layer\n",
    "    new_model = tf.keras.Sequential()\n",
    "    new_model.add(tf.keras.layers.Input(shape=(224,224,3), dtype='float32'))\n",
    "    for layer in model.layers[:-1]:\n",
    "        new_model.add(layer)\n",
    "    new_model.add(tf.keras.layers.Dense(output_size,activation='sigmoid'))\n",
    "\n",
    "    # Freeze all but num_active_layers layers\n",
    "    for layer_num, layer in enumerate(new_model.layers):\n",
    "        if layer_num < len(new_model.layers) - num_active_layers:\n",
    "            layer.trainable = False\n",
    "        else:\n",
    "            layer.trainable = True\n",
    "\n",
    "    # Setup optimizer, loss function, and metrics\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "    loss = 'binary_crossentropy'\n",
    "    metrics = ['binary_accuracy']\n",
    "    new_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    return new_model\n",
    "\n",
    "new_vgg_model = setup_model(vgg16, (224,224,3), 1, 7, 1e-4)\n",
    "new_vgg_model.summary()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup Model Callbacks\n",
    "Below is some helper code that will allow us to add callbacks to our model. This will save the 'best' version of the model\n",
    "by comparing it to previous epochs of training. The 'patience' parameter tells the code how long to wait without seeing\n",
    "any improvements to the chosen metric before quitting."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = \"{}_my_model.best.hdf5\".format('xray_class')\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(weight_path,\n",
    "                                                monitor= 'val_acc',\n",
    "                                                verbose=1,\n",
    "                                                save_best_only=True,\n",
    "                                                mode= 'max',\n",
    "                                                save_weights_only = True)\n",
    "\n",
    "early = tf.keras.callbacks.EarlyStopping(monitor= 'val_acc', mode= 'max', patience=15)\n",
    "\n",
    "callbacks_list = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "Everything is already set up for this, all we have to do is run the model and look at the results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_history = new_vgg_model.fit_generator(train_generator, validation_data = (val_x,val_y), epochs = 100, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance of Model\n",
    "#TODO Make note about how this is not that helpful in evaluting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Model History\n",
    "def plot_history(model_history):\n",
    "    N = len(model_history.history['loss'])\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0,N), model_history.history['loss'], label='train loss')\n",
    "    plt.plot(np.arange(0,N), model_history.history['val_loss'], label='valuation loss')\n",
    "    plt.plot(np.arange(0,N), model_history.history['binary_accuracy'], label='train accuracy')\n",
    "    plt.plot(np.arange(0,N), model_history.history['val_binary_accuracy'], label='valuation accuracy')\n",
    "    plt.title(\"Model Training Results\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.plot()\n",
    "\n",
    "plot_history(vgg_history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Look at Performance Statistics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_vgg_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-85f623455efb>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Get Model Predictions\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mnew_vgg_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_weights\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mweight_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mpred_Y\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnew_vgg_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mval_x\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m256\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mplot_auc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mt_y\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mp_y\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'new_vgg_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Get Model Predictions\n",
    "new_vgg_model.load_weights(weight_path)\n",
    "pred_Y = new_vgg_model.predict(val_x, batch_size = 11212, verbose = 1)\n",
    "\n",
    "def plot_roc_curve(model_name, true_y, pred_y):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(9,9))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(true_y, pred_y)\n",
    "    c_ax.plot(fpr, tpr, label=f'{model_name} AUC: {metrics.auc(fpr,tpr)}')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    return\n",
    "\n",
    "\n",
    "def plot_precision_recall_curve(model_name, true_y, pred_y):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(9,9))\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(true_y,pred_y)\n",
    "    ax.plot(recall, precision, label=f'{model_name} AP Score: {metrics.average_precision_score(true_y,pred_y)}')\n",
    "    plt.legend()\n",
    "    ax.set_xlabel(\"Recall\")\n",
    "    ax.set_ylabel(\"Precision\")\n",
    "    return\n",
    "\n",
    "def calculate_f1_scores(precision, recall):\n",
    "    return [(2*p*r)/(p+r) for p,r in zip(precision,recall)]\n",
    "\n",
    "def plot_f1_score(model_name, true_y, pred_y):\n",
    "    \"\"\" f1 = 2*(precision*recall) / (precision + recall)\"\"\"\n",
    "    fig, ax = plt.subplots(1,1,figsize=(9,9))\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(true_y,pred_y)\n",
    "    ax.plot(thresholds, precision, label=f'{model_name} Precision')\n",
    "    ax.plot(thresholds, recall, label=f'{model_name} Recall')\n",
    "    ax.plot(thresholds, calculate_f1_scores(precision, recall), label=f'{model_name} F1 Score')\n",
    "    plt.legend()\n",
    "    ax.set_xlabel(\"Threshold Values\")\n",
    "    return\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decide on classification threshold\n",
    "Once you feel you are done training, you'll need to decide the proper classification threshold that optimizes your model's\n",
    "performance for a given metric (e.g. accuracy, F1, precision, etc.  You decide)\n",
    "If this is a preliminary screening we want high recall so that we do not miss anyone that potentially has the disease.\n",
    "If the cost of the follow up exam is very expensive we want high precision because we do not want to waste patients money.\n",
    "If missing the exam leads to death we want high recall\n",
    "Maximize F1 score if you want to balance precision and recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "## Find the threshold that optimize your model's performance,\n",
    "## and use that threshold to make binary classification. Make sure you take all your metrics into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at some examples of predicted v. true with our best model: \n",
    "\n",
    "# Todo\n",
    "\n",
    "# fig, m_axs = plt.subplots(10, 10, figsize = (16, 16))\n",
    "# i = 0\n",
    "# for (c_x, c_y, c_ax) in zip(valX[0:100], testY[0:100], m_axs.flatten()):\n",
    "#     c_ax.imshow(c_x[:,:,0], cmap = 'bone')\n",
    "#     if c_y == 1: \n",
    "#         if pred_Y[i] > YOUR_THRESHOLD:\n",
    "#             c_ax.set_title('1, 1')\n",
    "#         else:\n",
    "#             c_ax.set_title('1, 0')\n",
    "#     else:\n",
    "#         if pred_Y[i] > YOUR_THRESHOLD: \n",
    "#             c_ax.set_title('0, 1')\n",
    "#         else:\n",
    "#             c_ax.set_title('0, 0')\n",
    "#     c_ax.axis('off')\n",
    "#     i=i+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save model architecture to a .json:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = new_model.to_json()\n",
    "with open(\"my_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}