{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Build and Train a Model\n",
    "In this notebook we preprocess the data to get it ready for a model, extend a pre-trained model from Keras, train the model on\n",
    "the x-ray images in our dataset, and evaluate model performance. The goal is the be able to accurately classify pneumonia with a\n",
    "reasonable level of accuracy. A good baseline accuracy for this task is ?, we hope to come close to this baseline\n",
    "and potentially be able to outperform it.\n",
    "\n",
    "TODO: Find the baseline accuracy from the article"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import Necessary Packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data\n",
    "In this section we preprocess the data to get it ready for our model. I parse the `Finding Label` to make create binary\n",
    "classification labels for each type of disease for every observation. I also create a `path` column for each `Image Index`\n",
    "so I can get to the location of the actual images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in the Data\n",
    "all_xray_df = pd.read_csv('/data/Data_Entry_2017.csv')\n",
    "\n",
    "# Convert `Finding Labels` to binary labels for each disease\n",
    "all_labels = np.unique(list(chain(*all_xray_df['Finding Labels'].map(lambda x: x.split('|')).tolist())))\n",
    "all_labels = [x for x in all_labels if len(x)>0]\n",
    "for label in all_labels:\n",
    "    if len(label)>1:\n",
    "        all_xray_df[label] = all_xray_df['Finding Labels'].map(lambda diseases: 1.0 if label in diseases else 0)\n",
    "\n",
    "# Create 'path' column for each 'Image Index'\n",
    "all_image_paths = {os.path.basename(x): x for x in\n",
    "                   glob(os.path.join('/data','images*', '*', '*.png'))}\n",
    "all_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\n",
    "\n",
    "# Check the results\n",
    "print(f'Images Found: {len(all_image_paths)}, Total Observations: {all_xray_df.shape[0]}')\n",
    "all_xray_df.sample(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_splits(vargs):\n",
    "    \n",
    "    ## Either build your own or use a built-in library to split your original dataframe into two sets \n",
    "    ## that can be used for training and testing your model\n",
    "    ## It's important to consider here how balanced or imbalanced you want each of those sets to be\n",
    "    ## for the presence of pneumonia\n",
    "\n",
    "    return train_data, val_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Augmentation and data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_image_augmentation(vargs):\n",
    "    \n",
    "    ## recommendation here to implement a package like Keras' ImageDataGenerator\n",
    "    ## with some of the built-in augmentations \n",
    "    \n",
    "    ## keep an eye out for types of augmentation that are or are not appropriate for medical imaging data\n",
    "    ## Also keep in mind what sort of augmentation is or is not appropriate for testing vs validation data\n",
    "\n",
    "    \n",
    "    # Todo\n",
    "    \n",
    "    return my_idg\n",
    "\n",
    "\n",
    "def make_train_gen(vargs):\n",
    "    \n",
    "    ## Create the actual generators using the output of my_image_augmentation for your training data\n",
    "    ## Suggestion here to use the flow_from_dataframe library, e.g.:\n",
    "    \n",
    "#     train_gen = my_train_idg.flow_from_dataframe(dataframe=train_df, \n",
    "#                                          directory=None, \n",
    "#                                          x_col = ,\n",
    "#                                          y_col = ,\n",
    "#                                          class_mode = 'binary',\n",
    "#                                          target_size = , \n",
    "#                                          batch_size = \n",
    "#                                          )\n",
    "     # Todo\n",
    "\n",
    "    return train_gen\n",
    "\n",
    "\n",
    "def make_val_gen(vargs):\n",
    "    \n",
    "#     val_gen = my_val_idg.flow_from_dataframe(dataframe = val_data, \n",
    "#                                              directory=None, \n",
    "#                                              x_col = ,\n",
    "#                                              y_col = ',\n",
    "#                                              class_mode = 'binary',\n",
    "#                                              target_size = , \n",
    "#                                              batch_size = ) \n",
    "    \n",
    "    # Todo\n",
    "    return val_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO: Isn't this already a part of the validation generator??? or the model training?\n",
    "## May want to pull a single large batch of random validation data for testing after each epoch:\n",
    "valX, valY = val_gen.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO: Move this up to underneath the data augmentation!\n",
    "## May want to look at some examples of our augmented training data. \n",
    "## This is helpful for understanding the extent to which data is being manipulated prior to training, \n",
    "## and can be compared with how the raw data look prior to augmentation\n",
    "\n",
    "t_x, t_y = next(train_gen)\n",
    "fig, m_axs = plt.subplots(4, 4, figsize = (16, 16))\n",
    "for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n",
    "    c_ax.imshow(c_x[:,:,0], cmap = 'bone')\n",
    "    if c_y == 1: \n",
    "        c_ax.set_title('Pneumonia')\n",
    "    else:\n",
    "        c_ax.set_title('No Pneumonia')\n",
    "    c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending a Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write this method for loading in VGG16 (Maybe make modular to load in other pretrained models\n",
    "def load_pretrained_model(vargs):\n",
    "    \n",
    "    # model = VGG16(include_top=True, weights='imagenet')\n",
    "    # transfer_layer = model.get_layer(lay_of_interest)\n",
    "    # vgg_model = Model(inputs = model.input, outputs = transfer_layer.output)\n",
    "\n",
    "    return vgg_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write model architecture\n",
    "def build_my_model(vargs):\n",
    "    \n",
    "    # my_model = Sequential()\n",
    "    # ....add your pre-trained model, and then whatever additional layers you think you might\n",
    "    # want for fine-tuning (Flatteen, Dense, Dropout, etc.)\n",
    "    \n",
    "    # if you want to compile your model within this function, consider which layers of your pre-trained model, \n",
    "    # you want to freeze before you compile \n",
    "    \n",
    "    # also make sure you set your optimizer, loss function, and metrics to monitor\n",
    "\n",
    "    return my_model\n",
    "\n",
    "\n",
    "\n",
    "## STAND-OUT Suggestion: choose another output layer besides just the last classification layer of your modele\n",
    "## to output class activation maps to aid in clinical interpretation of your model's results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Understand how this works and set it up\n",
    "## Below is some helper code that will allow you to add checkpoints to your model,\n",
    "## This will save the 'best' version of your model by comparing it to previous epochs of training\n",
    "\n",
    "## Note that you need to choose which metric to monitor for your model's 'best' performance if using this code. \n",
    "## The 'patience' parameter is set to 10, meaning that your model will train for ten epochs without seeing\n",
    "## improvement before quitting\n",
    "\n",
    "# Todo\n",
    "\n",
    "# weight_path=\"{}_my_model.best.hdf5\".format('xray_class')\n",
    "\n",
    "# checkpoint = ModelCheckpoint(weight_path, \n",
    "#                              monitor= CHOOSE_METRIC_TO_MONITOR_FOR_PERFORMANCE, \n",
    "#                              verbose=1, \n",
    "#                              save_best_only=True, \n",
    "#                              mode= CHOOSE_MIN_OR_MAX_FOR_YOUR_METRIC, \n",
    "#                              save_weights_only = True)\n",
    "\n",
    "# early = EarlyStopping(monitor= SAME_AS_METRIC_CHOSEN_ABOVE, \n",
    "#                       mode= CHOOSE_MIN_OR_MAX_FOR_YOUR_METRIC, \n",
    "#                       patience=10)\n",
    "\n",
    "# callbacks_list = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Setup model training\n",
    "\n",
    "# history = my_model.fit_generator(train_gen, \n",
    "#                           validation_data = (valX, valY), \n",
    "#                           epochs = , \n",
    "#                           callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate Performance of Model\n",
    "\n",
    "Note, these figures will come in handy for your FDA documentation later in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After training, make some predictions to assess your model's overall performance\n",
    "## Note that detecting pneumonia is hard even for trained expert radiologists, \n",
    "## so there is no need to make the model perfect.\n",
    "my_model.load_weights(weight_path)\n",
    "pred_Y = new_model.predict(valX, batch_size = 32, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc(t_y, p_y):\n",
    "    \n",
    "    ## Hint: can use scikit-learn's built in functions here like roc_curve\n",
    "    \n",
    "    # Todo\n",
    "    \n",
    "    return\n",
    "\n",
    "## what other performance statistics do you want to include here besides AUC? \n",
    "\n",
    "\n",
    "# def ... \n",
    "# Todo\n",
    "\n",
    "# def ...\n",
    "# Todo\n",
    "    \n",
    "#Also consider plotting the history of your model training:\n",
    "\n",
    "def plot_history(history):\n",
    "    \n",
    "    # Todo\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot figures\n",
    "\n",
    "# Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you feel you are done training, you'll need to decide the proper classification threshold that optimizes your model's performance for a given metric (e.g. accuracy, F1, precision, etc.  You decide) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the threshold that optimize your model's performance,\n",
    "## and use that threshold to make binary classification. Make sure you take all your metrics into consideration.\n",
    "\n",
    "# Todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at some examples of predicted v. true with our best model: \n",
    "\n",
    "# Todo\n",
    "\n",
    "# fig, m_axs = plt.subplots(10, 10, figsize = (16, 16))\n",
    "# i = 0\n",
    "# for (c_x, c_y, c_ax) in zip(valX[0:100], testY[0:100], m_axs.flatten()):\n",
    "#     c_ax.imshow(c_x[:,:,0], cmap = 'bone')\n",
    "#     if c_y == 1: \n",
    "#         if pred_Y[i] > YOUR_THRESHOLD:\n",
    "#             c_ax.set_title('1, 1')\n",
    "#         else:\n",
    "#             c_ax.set_title('1, 0')\n",
    "#     else:\n",
    "#         if pred_Y[i] > YOUR_THRESHOLD: \n",
    "#             c_ax.set_title('0, 1')\n",
    "#         else:\n",
    "#             c_ax.set_title('0, 0')\n",
    "#     c_ax.axis('off')\n",
    "#     i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just save model architecture to a .json:\n",
    "\n",
    "model_json = my_model.to_json()\n",
    "with open(\"my_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}