{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Build and Train a Model\n",
    "In this notebook we preprocess the data to get it ready for a model, extend a pre-trained model from Keras, train the model on\n",
    "the x-ray images in our dataset, and evaluate model performance. The goal is the be able to accurately classify pneumonia with a\n",
    "reasonable level of accuracy. A good baseline accuracy for this task is ?, we hope to come close to this baseline\n",
    "and potentially be able to outperform it.\n",
    "\n",
    "TODO: Find the baseline accuracy from the article"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import Necessary Packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "from random import sample\n",
    "from sklearn import model_selection\n",
    "import tensorflow as tf\n",
    "from skimage import io\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Data\n",
    "In this section we setup the data to get it ready for our model. I parse the `Finding Label` to make create binary\n",
    "classification labels for each type of disease for every observation. I also create a `path` column for each `Image Index`\n",
    "so I can get to the location of the actual images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in the Data\n",
    "all_xray_df = pd.read_csv('/data/Data_Entry_2017.csv')\n",
    "\n",
    "# Convert `Finding Labels` to binary labels for each disease\n",
    "all_labels = np.unique(list(chain(*all_xray_df['Finding Labels'].map(lambda x: x.split('|')).tolist())))\n",
    "all_labels = [x for x in all_labels if len(x)>0]\n",
    "for label in all_labels:\n",
    "    if len(label)>1:\n",
    "        all_xray_df[label] = all_xray_df['Finding Labels'].map(lambda diseases: 1.0 if label in diseases else 0)\n",
    "\n",
    "# Create 'path' column for each 'Image Index'\n",
    "all_image_paths = {os.path.basename(x): x for x in\n",
    "                   glob(os.path.join('/data','images*', '*', '*.png'))}\n",
    "all_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\n",
    "\n",
    "# Check the results\n",
    "print(f'Images Found: {len(all_image_paths)}, Total Observations: {all_xray_df.shape[0]}')\n",
    "all_xray_df.sample(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Validation Sets\n",
    "In this section I create the training and validation sets for the model. I am using an 90/10 split of the data with\n",
    "90% of the positive pneumonia cases in the training set and 10% of the positive pneumonia cases in the validation set.\n",
    "Also for the training set the proportion of positive to negative pneumonia cases will be 50/50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_xray_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-321c498bf99f>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     29\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mtrain_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 31\u001B[1;33m \u001B[0mtrain_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcreate_splits\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mall_xray_df\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0.2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'all_xray_df' is not defined"
     ]
    }
   ],
   "source": [
    "def create_splits(df, validation_percent):\n",
    "    # create original train / validation split\n",
    "    train_data, val_data = model_selection.train_test_split(df, test_size=validation_percent, stratify=df['Pneumonia'])\n",
    "\n",
    "    # Update training set to be 50/50 split\n",
    "    pos_indexes = train_data[train_data.Pneumonia == 1].index.tolist()\n",
    "    neg_indexes = train_data[train_data.Pneumonia == 0].index.tolist()\n",
    "    neg_sample = sample(neg_indexes, len(pos_indexes))\n",
    "    train_data = train_data.loc[pos_indexes+neg_sample]\n",
    "\n",
    "    # Check splits\n",
    "    print(f'Total Pneumonia Cases: {df[df.Pneumonia==1].shape[0]} \\\n",
    "    {(1-validation_percent)*100}% Pneumonia Cases: {int(df[df.Pneumonia==1].shape[0]*(1-validation_percent))} \\\n",
    "    {validation_percent*100}% Pneumonia Cases: {int(df[df.Pneumonia==1].shape[0]*validation_percent)}')\n",
    "    print(f'Pneumonia Cases in Training set: {train_data[train_data.Pneumonia==1].shape[0]} \\\n",
    "    Pneumonia Cases in Validation set: {val_data[val_data.Pneumonia==1].shape[0]}')\n",
    "\n",
    "    print(f'Train Set Size: {train_data.shape[0]}, \\\n",
    "    Pos %: {train_data[train_data.Pneumonia==1].shape[0] / train_data.shape[0] *100}, \\\n",
    "    Neg %: {train_data[train_data.Pneumonia==0].shape[0] / train_data.shape[0] *100}')\n",
    "\n",
    "    print(f'Original Set Size: {df.shape[0]} \\\n",
    "    Pos %: {df[df.Pneumonia==1].shape[0] / df.shape[0] *100} \\\n",
    "    Neg %: {df[df.Pneumonia==0].shape[0] / df.shape[0] *100}')\n",
    "\n",
    "    print(f'Validation Set Size: {val_data.shape[0]}, \\\n",
    "    Pos %: {val_data[val_data.Pneumonia == 1].shape[0] / val_data.shape[0] *100}, \\\n",
    "    Neg % {val_data[val_data.Pneumonia == 0].shape[0] / val_data.shape[0] *100}')\n",
    "\n",
    "    return train_data, val_data\n",
    "\n",
    "train_df, val_df = create_splits(all_xray_df, 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Image Augmentation and Data Generators\n",
    "In this section I set up image augmentation to generate a more diverse training set, and create training and validation\n",
    "set generators to use for model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (256, 256)\n",
    "\n",
    "# Create Image Data Generator for training set\n",
    "train_idg = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                         horizontal_flip = True,\n",
    "                         vertical_flip = False,\n",
    "                         rotation_range = 0.5,\n",
    "                         shear_range = 0.1,\n",
    "                         zoom_range = 0.15)\n",
    "\n",
    "train_generator = train_idg.flow_from_dataframe(dataframe=train_df, directory=None,\n",
    "                                          x_col='path', y_col='Pneumonia',\n",
    "                                          class_mode = 'binary', target_size=IMG_SIZE, batch_size=24)\n",
    "\n",
    "# Create a generator for the test set\n",
    "validation_idg = ImageDataGenerator(rescale=1.0/255.0)\n",
    "validation_generator = validation_idg.flow_from_dataframe(dataframe=val_df, directory=None,\n",
    "                                                      x_col='path', y_col='Pneumonia',\n",
    "                                                      class_mode='binary', target_size=IMG_SIZE, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a random batch of training and validation data to make sure everything looks okay\n",
    "# Training Examples\n",
    "t_x, t_y = next(train_generator)\n",
    "fig, m_axs = plt.subplots(4, 4, figsize = (16, 16))\n",
    "for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n",
    "    c_ax.imshow(c_x[:,:,0], cmap = 'bone')\n",
    "    if c_y == 1: \n",
    "        c_ax.set_title('Pneumonia')\n",
    "    else:\n",
    "        c_ax.set_title('No Pneumonia')\n",
    "    c_ax.axis('off')\n",
    "\n",
    "# Validation Examples\n",
    "t_x, t_y = next(validation_generator)\n",
    "fig, m_axs = plt.subplots(2, 2, figsize = (16, 16))\n",
    "for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n",
    "    c_ax.imshow(c_x[:,:,0], cmap = 'bone')\n",
    "    if c_y == 1:\n",
    "        c_ax.set_title('Pneumonia')\n",
    "    else:\n",
    "        c_ax.set_title('No Pneumonia')\n",
    "    c_ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending a Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write this method for loading in VGG16 (Maybe make modular to load in other pretrained models\n",
    "def load_pretrained_model(vargs):\n",
    "    \n",
    "    # model = VGG16(include_top=True, weights='imagenet')\n",
    "    # transfer_layer = model.get_layer(lay_of_interest)\n",
    "    # vgg_model = Model(inputs = model.input, outputs = transfer_layer.output)\n",
    "\n",
    "    return vgg_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write model architecture\n",
    "def build_my_model(vargs):\n",
    "    \n",
    "    # my_model = Sequential()\n",
    "    # ....add your pre-trained model, and then whatever additional layers you think you might\n",
    "    # want for fine-tuning (Flatteen, Dense, Dropout, etc.)\n",
    "    \n",
    "    # if you want to compile your model within this function, consider which layers of your pre-trained model, \n",
    "    # you want to freeze before you compile \n",
    "    \n",
    "    # also make sure you set your optimizer, loss function, and metrics to monitor\n",
    "\n",
    "    return my_model\n",
    "\n",
    "\n",
    "\n",
    "## STAND-OUT Suggestion: choose another output layer besides just the last classification layer of your modele\n",
    "## to output class activation maps to aid in clinical interpretation of your model's results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Understand how this works and set it up\n",
    "## Below is some helper code that will allow you to add checkpoints to your model,\n",
    "## This will save the 'best' version of your model by comparing it to previous epochs of training\n",
    "\n",
    "## Note that you need to choose which metric to monitor for your model's 'best' performance if using this code. \n",
    "## The 'patience' parameter is set to 10, meaning that your model will train for ten epochs without seeing\n",
    "## improvement before quitting\n",
    "\n",
    "# Todo\n",
    "\n",
    "# weight_path=\"{}_my_model.best.hdf5\".format('xray_class')\n",
    "\n",
    "# checkpoint = ModelCheckpoint(weight_path, \n",
    "#                              monitor= CHOOSE_METRIC_TO_MONITOR_FOR_PERFORMANCE, \n",
    "#                              verbose=1, \n",
    "#                              save_best_only=True, \n",
    "#                              mode= CHOOSE_MIN_OR_MAX_FOR_YOUR_METRIC, \n",
    "#                              save_weights_only = True)\n",
    "\n",
    "# early = EarlyStopping(monitor= SAME_AS_METRIC_CHOSEN_ABOVE, \n",
    "#                       mode= CHOOSE_MIN_OR_MAX_FOR_YOUR_METRIC, \n",
    "#                       patience=10)\n",
    "\n",
    "# callbacks_list = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Setup model training\n",
    "\n",
    "# history = my_model.fit_generator(train_gen, \n",
    "#                           validation_data = (valX, valY), \n",
    "#                           epochs = , \n",
    "#                           callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate Performance of Model\n",
    "\n",
    "Note, these figures will come in handy for your FDA documentation later in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After training, make some predictions to assess your model's overall performance\n",
    "## Note that detecting pneumonia is hard even for trained expert radiologists, \n",
    "## so there is no need to make the model perfect.\n",
    "my_model.load_weights(weight_path)\n",
    "pred_Y = new_model.predict(valX, batch_size = 32, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc(t_y, p_y):\n",
    "    \n",
    "    ## Hint: can use scikit-learn's built in functions here like roc_curve\n",
    "    \n",
    "    # Todo\n",
    "    \n",
    "    return\n",
    "\n",
    "## what other performance statistics do you want to include here besides AUC? \n",
    "\n",
    "\n",
    "# def ... \n",
    "# Todo\n",
    "\n",
    "# def ...\n",
    "# Todo\n",
    "    \n",
    "#Also consider plotting the history of your model training:\n",
    "\n",
    "def plot_history(history):\n",
    "    \n",
    "    # Todo\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot figures\n",
    "\n",
    "# Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you feel you are done training, you'll need to decide the proper classification threshold that optimizes your model's performance for a given metric (e.g. accuracy, F1, precision, etc.  You decide) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the threshold that optimize your model's performance,\n",
    "## and use that threshold to make binary classification. Make sure you take all your metrics into consideration.\n",
    "\n",
    "# Todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at some examples of predicted v. true with our best model: \n",
    "\n",
    "# Todo\n",
    "\n",
    "# fig, m_axs = plt.subplots(10, 10, figsize = (16, 16))\n",
    "# i = 0\n",
    "# for (c_x, c_y, c_ax) in zip(valX[0:100], testY[0:100], m_axs.flatten()):\n",
    "#     c_ax.imshow(c_x[:,:,0], cmap = 'bone')\n",
    "#     if c_y == 1: \n",
    "#         if pred_Y[i] > YOUR_THRESHOLD:\n",
    "#             c_ax.set_title('1, 1')\n",
    "#         else:\n",
    "#             c_ax.set_title('1, 0')\n",
    "#     else:\n",
    "#         if pred_Y[i] > YOUR_THRESHOLD: \n",
    "#             c_ax.set_title('0, 1')\n",
    "#         else:\n",
    "#             c_ax.set_title('0, 0')\n",
    "#     c_ax.axis('off')\n",
    "#     i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just save model architecture to a .json:\n",
    "\n",
    "model_json = my_model.to_json()\n",
    "with open(\"my_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}